{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rapid-director",
   "metadata": {},
   "source": [
    "## Multimodal Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "brilliant-combining",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "operational-facing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adopted from:\n",
    "# Multi-Domain Translation between Single-Cell Imaging and Sequencing Data using Autoencoders\n",
    "# https://github.com/uhlerlab/cross-modal-autoencoders\n",
    "\n",
    "class FC_Autoencoder(nn.Module):\n",
    "    def __init__(self, n_input, latent_variable_size, n_hidden=512):\n",
    "        super(FC_Autoencoder, self).__init__()\n",
    "        self.latent_variable_size = latent_variable_size\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.n_input, n_hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.fc1 = nn.Linear(n_hidden, latent_variable_size)\n",
    "        self.fc2 = nn.Linear(n_hidden, latent_variable_size)\n",
    "\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_variable_size, n_hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(n_hidden, n_input),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        latent_space = self.reparametrize(mu, logvar)\n",
    "        res = self.decode(latent_space)\n",
    "        return res, latent_space, mu, logvar\n",
    "    \n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)       \n",
    "        return self.fc1(h), self.fc2(h)\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        if torch.cuda.is_available():\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def get_latent_var(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return z\n",
    " \n",
    "    def generate(self, z):\n",
    "        res = self.decode(z)\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "liable-county",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess (Essentiality, Expression):\n",
    "\n",
    "    dataset_processed = dict()\n",
    "    common_cells = set(Expression.index)  & set(Essentiality.index)\n",
    "    for cell in common_cells:\n",
    "\n",
    "        dataset_processed[cell] = {\n",
    "            \"data_ess\": torch.tensor(Essentiality.loc[cell]),\n",
    "            \"data_exp\": torch.tensor(Expression.loc[cell])\n",
    "        }\n",
    "        \n",
    "    ess_gene_list, exp_gene_list = list(Essentiality.columns), list(Expression.columns)\n",
    "    return dataset_processed, ess_gene_list, exp_gene_list\n",
    "\n",
    "\n",
    "class JointDataset(Dataset):\n",
    "    def __init__(self, dataset, from_keys):\n",
    "        \n",
    "        self.data_ess = []\n",
    "        self.data_exp = []\n",
    "        self.cell_line = []\n",
    "\n",
    "        self.keys = from_keys\n",
    "        \n",
    "        for cell in from_keys: #dataset.keys():\n",
    "            self.data_ess.append(dataset[cell][\"data_ess\"])\n",
    "            self.data_exp.append(dataset[cell][\"data_exp\"])\n",
    "            self.cell_line.append(cell)\n",
    "    \n",
    "        self.data_ess = torch.stack(self.data_ess).float()\n",
    "        self.data_exp = torch.stack(self.data_exp).float()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data_ess[index], self.data_exp[index], self.cell_line[index]\n",
    "    \n",
    "\n",
    "def load_data(batch_size = None, val_split=False,drug=False):\n",
    "    \n",
    "    if drug:\n",
    "\n",
    "        Essentiality = pd.read_csv(\"data/primary-screen-replicate-collapsed-logfold-change.csv\"\n",
    "                                  ).rename(columns={\"Unnamed: 0\": 'CellLine'}).set_index('CellLine')\n",
    "        Essentiality = Essentiality.fillna(Essentiality.mean())\n",
    "        Essentiality.columns = Essentiality.columns.str.replace(r'[^a-zA-Z0-9]', '.', regex=True)\n",
    "\n",
    "        with open(\"data/top500_variable_drug.txt\", 'r') as file:\n",
    "            variable_drug = file.read().splitlines()\n",
    "            \n",
    "        Essentiality = Essentiality[variable_drug]\n",
    "        \n",
    "    else:\n",
    "\n",
    "        Essentiality = pd.read_csv(\"data/CRISPRGeneEffect.csv\"\n",
    "                                  ).rename(columns={\"ModelID\":\n",
    "                                                    'CellLine'}).set_index('CellLine')  \n",
    "        \n",
    "        Essentiality.columns = [col.split(\" \")[0] for col in Essentiality.columns]\n",
    "\n",
    "\n",
    "    Expression = pd.read_csv(\"data/OmicsExpressionProteinCodingGenesTPMLogp1.csv\").rename(\n",
    "        columns={\"Unnamed: 0\": 'CellLine'}).set_index('CellLine')\n",
    "    Expression.columns = [col.split(\" \")[0] for col in Expression.columns]\n",
    "    \n",
    "    Joint_data, ess_gene_list, exp_gene_list = preprocess(Essentiality, Expression)\n",
    "\n",
    "    random_cell_line = random.choice(list(Joint_data.keys()))\n",
    "    n_input_ess = Joint_data[random_cell_line][\"data_ess\"].size(0)\n",
    "    n_input_exp = Joint_data[random_cell_line][\"data_exp\"].size(0)\n",
    "\n",
    "    if val_split:\n",
    "        train_cells, val_cells = train_test_split(list(Joint_data.keys()), test_size=0.3, random_state=42)\n",
    "        val_cells, test_cells = train_test_split(val_cells, test_size=0.5, random_state=42)\n",
    "    \n",
    "        train_data = JointDataset(Joint_data, train_cells)\n",
    "        val_data =  JointDataset(Joint_data, val_cells)\n",
    "        test_data =  JointDataset(Joint_data, test_cells)\n",
    "        \n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
    "        val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    else:\n",
    "        train_cells, test_cells = train_test_split(list(Joint_data.keys()), test_size=0.2, random_state=42)\n",
    "        \n",
    "        train_data = JointDataset(Joint_data, train_cells)\n",
    "        test_data =  JointDataset(Joint_data, test_cells)\n",
    "\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "        val_loader = None\n",
    "        val_data = []\n",
    "\n",
    "    print(f\"Split:{len(train_data)}, Size of Val:{len(val_data)}, Size of Test: {len(test_data)}\")\n",
    "    print(f\"Essentiality: {train_loader.dataset[0][0].shape} x {len(ess_gene_list)}\")\n",
    "    print(f\"Expression: {train_loader.dataset[0][1].shape} x {len(exp_gene_list)}\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader, n_input_ess, n_input_exp, ess_gene_list, exp_gene_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-slope",
   "metadata": {},
   "source": [
    "### Model definitions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "divided-eclipse",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_KL_loss(mu, logvar,lambda_=1e-08):\n",
    "    if lambda_>0:\n",
    "        KLloss = -0.5*torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return lambda_ * KLloss\n",
    "    \n",
    "    return 0\n",
    "\n",
    "def train(loader, model_ess, model_exp, opt=\"adam\", alpha=0.5, beta=0.5, lr= 1e-3, device=\"cuda\"):\n",
    "    \n",
    "    model_ess.train()\n",
    "    model_exp.train()\n",
    "\n",
    "    MSELoss = nn.MSELoss()\n",
    "\n",
    "    parameters = list(model_ess.parameters()) + list(model_exp.parameters())\n",
    "    if opt.lower() == \"adam\":\n",
    "        optimizer = optim.Adam(parameters, lr=lr)\n",
    "        \n",
    "    elif opt.lower() == \"adamw\":\n",
    "        optimizer = optim.AdamW(parameters, lr=lr)\n",
    "        \n",
    "    elif opt.lower() == \"adagrad\":\n",
    "        optimizer = optim.Adagrad(parameters, lr=lr)\n",
    "        \n",
    "    elif opt.lower() == \"sgd\":\n",
    "        optimizer = optim.SGD(parameters, lr=lr)\n",
    "    else:\n",
    "        raise ValueError(f\"Optimizer value error :{opt}\")\n",
    "       \n",
    "    train_loss = 0\n",
    "    for batch in loader:\n",
    "        \n",
    "        data_ess, data_exp = batch[0].to(device) , batch[1].to(device)\n",
    "        \n",
    "        #Reconstruction\n",
    "        decoded_ess, _ , mu_ess, logvar_ess = model_ess(data_ess)\n",
    "        decoded_exp, latent_exp, mu_exp, logvar_exp = model_exp(data_exp)\n",
    "\n",
    "        loss_ess = MSELoss(data_ess, decoded_ess)\n",
    "        loss_exp = MSELoss(data_exp, decoded_exp)\n",
    "\n",
    "        #Decoding from latent space\n",
    "        pred_ess = model_ess.decode(latent_exp)\n",
    "        loss_pred = MSELoss (data_ess, pred_ess )\n",
    "\n",
    "        #Kl-Loss\n",
    "        kl_loss = compute_KL_loss(mu_ess, logvar_ess) + compute_KL_loss(mu_exp, logvar_exp)\n",
    "        \n",
    "        \n",
    "        total_loss = alpha*(loss_ess + loss_exp) + (beta)* loss_pred + (1-alpha-beta)* kl_loss\n",
    "       \n",
    "        optimizer.zero_grad()        \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += total_loss.item()\n",
    "    \n",
    "    train_loss /= len(loader)\n",
    "    return train_loss, model_ess, model_exp\n",
    "\n",
    "\n",
    "def evaluate(loader, model_ess, model_exp, device=\"cuda\"):\n",
    "    \n",
    "    model_ess.eval()\n",
    "    model_exp.eval() \n",
    "    MSELoss = nn.MSELoss()\n",
    "\n",
    "    y_ess_pred = []\n",
    "    y_ess_true = []\n",
    "\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            \n",
    "            data_ess, data_exp = batch[0].to(device) , batch[1].to(device)\n",
    "            _, latent_exp, _, _ = model_exp(data_exp)\n",
    "            pred_ess = model_ess.decode(latent_exp)\n",
    "\n",
    "            total_loss = MSELoss(data_ess, pred_ess)\n",
    "            val_loss += total_loss.item()\n",
    "\n",
    "            y_ess_pred.append(pred_ess)\n",
    "            y_ess_true.append(data_ess)\n",
    "    y_ess_pred = torch.cat(y_ess_pred, dim=0).detach().cpu().numpy()\n",
    "    y_ess_true = torch.cat(y_ess_true, dim=0).detach().cpu().numpy()\n",
    "\n",
    "    val_loss /= len(loader)\n",
    "    return val_loss, y_ess_pred, y_ess_true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "better-transcription",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3469976/3479486273.py:8: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"data_ess\": torch.tensor(Essentiality.loc[cell]),\n",
      "/tmp/ipykernel_3469976/3479486273.py:9: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  \"data_exp\": torch.tensor(Expression.loc[cell])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split:390, Size of Val:84, Size of Test: 84\n",
      "Essentiality: torch.Size([500]) x 500\n",
      "Expression: torch.Size([19193]) x 19193\n",
      "Epoch 1 of 50\n",
      "-------------------------------\n",
      "\tET: 1.0977818965911865\n",
      "\n",
      "\tTrain Loss: 10.292876815795898\n",
      "\n",
      "\tVal Loss: 5.623782793680827\n",
      "\n",
      "Epoch 2 of 50\n",
      "-------------------------------\n",
      "\tET: 1.3361341953277588\n",
      "\n",
      "\tTrain Loss: 9.301541805267334\n",
      "\n",
      "\tVal Loss: 4.933698336283366\n",
      "\n",
      "Epoch 3 of 50\n",
      "-------------------------------\n",
      "\tET: 1.33441162109375\n",
      "\n",
      "\tTrain Loss: 8.374310207366943\n",
      "\n",
      "\tVal Loss: 4.149767239888509\n",
      "\n",
      "Epoch 4 of 50\n",
      "-------------------------------\n",
      "\tET: 1.3264520168304443\n",
      "\n",
      "\tTrain Loss: 7.499209594726563\n",
      "\n",
      "\tVal Loss: 3.6813601652781167\n",
      "\n",
      "Epoch 5 of 50\n",
      "-------------------------------\n",
      "\tET: 0.8791179656982422\n",
      "\n",
      "\tTrain Loss: 6.703956413269043\n",
      "\n",
      "\tVal Loss: 3.85650634765625\n",
      "\n",
      "Epoch 6 of 50\n",
      "-------------------------------\n",
      "\tET: 0.8498952388763428\n",
      "\n",
      "\tTrain Loss: 6.006777238845825\n",
      "\n",
      "\tVal Loss: 3.948876221974691\n",
      "\n",
      "Epoch 7 of 50\n",
      "-------------------------------\n",
      "\tET: 1.3438818454742432\n",
      "\n",
      "\tTrain Loss: 5.383936977386474\n",
      "\n",
      "\tVal Loss: 2.486497243245443\n",
      "\n",
      "Epoch 8 of 50\n",
      "-------------------------------\n",
      "\tET: 0.8630046844482422\n",
      "\n",
      "\tTrain Loss: 4.821866846084594\n",
      "\n",
      "\tVal Loss: 3.4044195810953775\n",
      "\n",
      "Epoch 9 of 50\n",
      "-------------------------------\n",
      "\tET: 0.8455719947814941\n",
      "\n",
      "\tTrain Loss: 4.321154975891114\n",
      "\n",
      "\tVal Loss: 3.183394511540731\n",
      "\n",
      "Epoch 10 of 50\n",
      "-------------------------------\n",
      "\tET: 1.3640666007995605\n",
      "\n",
      "\tTrain Loss: 3.8760889291763307\n",
      "\n",
      "\tVal Loss: 2.147951324780782\n",
      "\n",
      "Epoch 11 of 50\n",
      "-------------------------------\n",
      "\tET: 0.8704395294189453\n",
      "\n",
      "\tTrain Loss: 3.479564905166626\n",
      "\n",
      "\tVal Loss: 2.8742433389027915\n",
      "\n",
      "Epoch 12 of 50\n",
      "-------------------------------\n",
      "\tET: 1.3301072120666504\n",
      "\n",
      "\tTrain Loss: 3.1229251861572265\n",
      "\n",
      "\tVal Loss: 1.6733115911483765\n",
      "\n",
      "Epoch 13 of 50\n",
      "-------------------------------\n",
      "\tET: 1.2982401847839355\n",
      "\n",
      "\tTrain Loss: 2.822145700454712\n",
      "\n",
      "\tVal Loss: 1.5255495309829712\n",
      "\n",
      "Epoch 14 of 50\n",
      "-------------------------------\n",
      "\tET: 0.868009090423584\n",
      "\n",
      "\tTrain Loss: 2.592532777786255\n",
      "\n",
      "\tVal Loss: 1.6734652519226074\n",
      "\n",
      "Epoch 15 of 50\n",
      "-------------------------------\n",
      "\tET: 1.351203441619873\n",
      "\n",
      "\tTrain Loss: 2.3716180086135865\n",
      "\n",
      "\tVal Loss: 1.5059754848480225\n",
      "\n",
      "Epoch 16 of 50\n",
      "-------------------------------\n",
      "\tET: 1.3492252826690674\n",
      "\n",
      "\tTrain Loss: 2.1880780458450317\n",
      "\n",
      "\tVal Loss: 1.4044185479482014\n",
      "\n",
      "Epoch 17 of 50\n",
      "-------------------------------\n",
      "\tET: 0.8461964130401611\n",
      "\n",
      "\tTrain Loss: 2.023390519618988\n",
      "\n",
      "\tVal Loss: 1.5457561016082764\n",
      "\n",
      "Epoch 18 of 50\n",
      "-------------------------------\n",
      "\tET: 1.390699863433838\n",
      "\n",
      "\tTrain Loss: 1.8543907046318053\n",
      "\n",
      "\tVal Loss: 1.3831831216812134\n",
      "\n",
      "Epoch 19 of 50\n",
      "-------------------------------\n",
      "\tET: 1.3050923347473145\n",
      "\n",
      "\tTrain Loss: 1.6911565065383911\n",
      "\n",
      "\tVal Loss: 1.3597978353500366\n",
      "\n",
      "Epoch 20 of 50\n",
      "-------------------------------\n",
      "\tET: 0.8933448791503906\n",
      "\n",
      "\tTrain Loss: 1.5610490798950196\n",
      "\n",
      "\tVal Loss: 1.7354213794072468\n",
      "\n",
      "Epoch 21 of 50\n",
      "-------------------------------\n",
      "\tET: 0.874182939529419\n",
      "\n",
      "\tTrain Loss: 1.464527952671051\n",
      "\n",
      "\tVal Loss: 1.7603058417638142\n",
      "\n",
      "Epoch 22 of 50\n",
      "-------------------------------\n",
      "\tET: 0.8574645519256592\n",
      "\n",
      "\tTrain Loss: 1.392512810230255\n",
      "\n",
      "\tVal Loss: 1.444548765818278\n",
      "\n",
      "Epoch 23 of 50\n",
      "-------------------------------\n",
      "\tET: 0.6699371337890625\n",
      "\n",
      "\tTrain Loss: 1.337994122505188\n",
      "\n",
      "\tVal Loss: 1.4716206789016724\n",
      "\n",
      "Epoch 24 of 50\n",
      "-------------------------------\n",
      "\tET: 0.745415449142456\n",
      "\n",
      "\tTrain Loss: 1.297871732711792\n",
      "\n",
      "\tVal Loss: 1.368114948272705\n",
      "\n",
      "Epoch 25 of 50\n",
      "-------------------------------\n",
      "\tET: 0.9007067680358887\n",
      "\n",
      "\tTrain Loss: 1.2725948572158814\n",
      "\n",
      "\tVal Loss: 1.7911475499471028\n",
      "\n",
      "Epoch 26 of 50\n",
      "-------------------------------\n",
      "\tET: 1.393580675125122\n",
      "\n",
      "\tTrain Loss: 1.2433398723602296\n",
      "\n",
      "\tVal Loss: 1.315218210220337\n",
      "\n",
      "Epoch 27 of 50\n",
      "-------------------------------\n",
      "\tET: 0.8879194259643555\n",
      "\n",
      "\tTrain Loss: 1.22243971824646\n",
      "\n",
      "\tVal Loss: 1.5140684843063354\n",
      "\n",
      "Epoch 28 of 50\n",
      "-------------------------------\n",
      "\tET: 0.8895721435546875\n",
      "\n",
      "\tTrain Loss: 1.2038069248199463\n",
      "\n",
      "\tVal Loss: 1.4140019814173381\n",
      "\n",
      "Epoch 29 of 50\n",
      "-------------------------------\n",
      "\tET: 0.8915774822235107\n",
      "\n",
      "\tTrain Loss: 1.1821680068969727\n",
      "\n",
      "\tVal Loss: 1.388016899426778\n",
      "\n",
      "Epoch 30 of 50\n",
      "-------------------------------\n",
      "\tET: 0.8803896903991699\n",
      "\n",
      "\tTrain Loss: 1.166813898086548\n",
      "\n",
      "\tVal Loss: 1.3557610114415486\n",
      "\n",
      "Epoch 31 of 50\n",
      "-------------------------------\n",
      "\tET: 0.8902339935302734\n",
      "\n",
      "\tTrain Loss: 1.1498294591903686\n",
      "\n",
      "\tVal Loss: 1.5324939489364624\n",
      "\n",
      "Epoch 32 of 50\n",
      "-------------------------------\n",
      "\tET: 0.8687033653259277\n",
      "\n",
      "\tTrain Loss: 1.1359566807746888\n",
      "\n",
      "\tVal Loss: 1.4392494757970173\n",
      "\n",
      "Epoch 33 of 50\n",
      "-------------------------------\n",
      "\tET: 0.8593442440032959\n",
      "\n",
      "\tTrain Loss: 1.1237639427185058\n",
      "\n",
      "\tVal Loss: 1.3187631766001384\n",
      "\n",
      "Epoch 34 of 50\n",
      "-------------------------------\n",
      "\tET: 0.730064868927002\n",
      "\n",
      "\tTrain Loss: 1.1129598677158357\n",
      "\n",
      "\tVal Loss: 1.3412105639775593\n",
      "\n",
      "Epoch 35 of 50\n",
      "-------------------------------\n",
      "\tET: 0.8611021041870117\n",
      "\n",
      "\tTrain Loss: 1.0999697268009185\n",
      "\n",
      "\tVal Loss: 1.4072586297988892\n",
      "\n",
      "Epoch 36 of 50\n",
      "-------------------------------\n",
      "\tET: 0.8929834365844727\n",
      "\n",
      "\tTrain Loss: 1.0929956138134003\n",
      "\n",
      "\tVal Loss: 1.3477540413538616\n",
      "\n",
      "Epoch 37 of 50\n",
      "-------------------------------\n",
      "\tET: 0.8373508453369141\n",
      "\n",
      "\tTrain Loss: 1.0861494541168213\n",
      "\n",
      "\tVal Loss: 1.3545479774475098\n",
      "\n",
      "Epoch 38 of 50\n",
      "-------------------------------\n",
      "\tET: 1.3638882637023926\n",
      "\n",
      "\tTrain Loss: 1.0711212277412414\n",
      "\n",
      "\tVal Loss: 1.3025381565093994\n",
      "\n",
      "Epoch 39 of 50\n",
      "-------------------------------\n",
      "\tET: 0.8714768886566162\n",
      "\n",
      "\tTrain Loss: 1.0685028910636902\n",
      "\n",
      "\tVal Loss: 1.4387240012486775\n",
      "\n",
      "Epoch 40 of 50\n",
      "-------------------------------\n",
      "\tET: 0.8976716995239258\n",
      "\n",
      "\tTrain Loss: 1.0539115130901338\n",
      "\n",
      "\tVal Loss: 1.3425755898157756\n",
      "\n",
      "Epoch 41 of 50\n",
      "-------------------------------\n",
      "\tET: 0.8499894142150879\n",
      "\n",
      "\tTrain Loss: 1.0483502089977264\n",
      "\n",
      "\tVal Loss: 1.3711144129435222\n",
      "\n",
      "Epoch 42 of 50\n",
      "-------------------------------\n",
      "\tET: 0.8940210342407227\n",
      "\n",
      "\tTrain Loss: 1.0392013370990754\n",
      "\n",
      "\tVal Loss: 1.3103632529576619\n",
      "\n",
      "Epoch 43 of 50\n",
      "-------------------------------\n",
      "\tET: 0.899266242980957\n",
      "\n",
      "\tTrain Loss: 1.0328779816627502\n",
      "\n",
      "\tVal Loss: 1.3347434202829997\n",
      "\n",
      "Epoch 44 of 50\n",
      "-------------------------------\n",
      "\tET: 0.8453571796417236\n",
      "\n",
      "\tTrain Loss: 1.0283659517765045\n",
      "\n",
      "\tVal Loss: 1.5649445056915283\n",
      "\n",
      "Epoch 45 of 50\n",
      "-------------------------------\n",
      "\tET: 0.6976807117462158\n",
      "\n",
      "\tTrain Loss: 1.0193177878856658\n",
      "\n",
      "\tVal Loss: 1.3745134671529133\n",
      "\n",
      "Epoch 46 of 50\n",
      "-------------------------------\n",
      "\tET: 0.8802087306976318\n",
      "\n",
      "\tTrain Loss: 1.0144621133804321\n",
      "\n",
      "\tVal Loss: 1.430682897567749\n",
      "\n",
      "Epoch 47 of 50\n",
      "-------------------------------\n",
      "\tET: 1.3840773105621338\n",
      "\n",
      "\tTrain Loss: 1.0072951912879944\n",
      "\n",
      "\tVal Loss: 1.2657320896784465\n",
      "\n",
      "Epoch 48 of 50\n",
      "-------------------------------\n",
      "\tET: 0.8545534610748291\n",
      "\n",
      "\tTrain Loss: 0.9988014280796051\n",
      "\n",
      "\tVal Loss: 1.3808062473932903\n",
      "\n",
      "Epoch 49 of 50\n",
      "-------------------------------\n",
      "\tET: 0.8698244094848633\n",
      "\n",
      "\tTrain Loss: 0.995138019323349\n",
      "\n",
      "\tVal Loss: 1.347235083580017\n",
      "\n",
      "Epoch 50 of 50\n",
      "-------------------------------\n",
      "\tET: 0.8506543636322021\n",
      "\n",
      "\tTrain Loss: 0.9872165083885193\n",
      "\n",
      "\tVal Loss: 1.3291091124216716\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "latent_size = int(10e3) #1024\n",
    "batch_size = 40\n",
    "lr = 0.0001\n",
    "epochs = 50\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "alpha = 0.4\n",
    "beta = 0.5\n",
    "opt=\"adam\"\n",
    "\n",
    "(\n",
    "    train_loader, val_loader, test_loader, n_input_ess, n_input_exp, ess_gene_list, exp_gene_list\n",
    ")= load_data(batch_size = batch_size, val_split=True, drug=True)\n",
    "\n",
    "model_ess = FC_Autoencoder(n_input_ess, latent_size).to(device)\n",
    "model_exp = FC_Autoencoder(n_input_exp, latent_size).to(device) \n",
    "\n",
    "\n",
    "optimal_loss = float('inf')\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "    print(\"-------------------------------\")\n",
    "    \n",
    "    train_loss, model_ess_t, model_exp_t = train(train_loader, model_ess, model_exp, opt=opt, alpha=alpha, beta=beta, lr=lr, device=device)\n",
    "    val_loss, y_ess_pred, y_ess_true = evaluate(val_loader, model_ess, model_exp, device=device)\n",
    "\n",
    "    if val_loss < optimal_loss:\n",
    "        optimal_loss = val_loss\n",
    "        torch.save(model_ess_t.state_dict(), \"baselines/VAE_baseline/model_ess_var.pt\")\n",
    "        torch.save(model_exp_t.state_dict(), \"baselines/VAE_baseline/model_exp_var.pt\")\n",
    "\n",
    "\n",
    "    print(f'\\tET: {time.time() - start_time}\\n')\n",
    "\n",
    "    print(f'\\tTrain Loss: {train_loss}\\n')\n",
    "    print(f'\\tVal Loss: {val_loss}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4ca1694c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def get_corrcoef (data1 , data2, dim=0, corr=\"spearman\"):\n",
    "    \n",
    "    if isinstance(data1, torch.Tensor):\n",
    "\n",
    "        if data1.is_cuda:\n",
    "            data1 = data1.cpu().detach().numpy()\n",
    "            data2 = data2.cpu().detach().numpy()\n",
    "        else:\n",
    "            data1 = data1.detach().numpy()\n",
    "            data2 = data2.detach().numpy()\n",
    "\n",
    "    elif isinstance(data1, pd.DataFrame):\n",
    "        data1 = data1.values\n",
    "        data2 = data2.values\n",
    "            \n",
    "    if corr.lower() == \"pearson\":\n",
    "        \n",
    "        if dim == 0:\n",
    "            corrcoef =[np.corrcoef(data1[:,i], data2[:,i])[0, 1] for i in range(data1.shape[1])]\n",
    "            corrcoef = np.array(corrcoef)\n",
    "            corrcoef = corrcoef[~np.isnan(corrcoef)]\n",
    "        elif dim == 1:\n",
    "            corrcoef =[np.corrcoef(data1[i,:], data2[i,:])[0, 1] for i in range(data1.shape[0])]\n",
    "            corrcoef = np.array(corrcoef)\n",
    "            corrcoef = corrcoef[~np.isnan(corrcoef)]\n",
    "        else:\n",
    "            raise ValueError(f\"dim value erro: dim={dim}\")\n",
    "        \n",
    "    elif corr.lower() == \"spearman\":\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            \n",
    "            if dim == 0:\n",
    "                corrcoef =[spearmanr(data1[:,i], data2[:,i])[0] for i in range(data1.shape[1])]\n",
    "                corrcoef = np.array(corrcoef)\n",
    "                corrcoef = corrcoef[~np.isnan(corrcoef)]\n",
    "            elif dim == 1:\n",
    "                corrcoef =[spearmanr(data1[i,:], data2[i,:])[0] for i in range(data1.shape[0])]\n",
    "                corrcoef = np.array(corrcoef)\n",
    "                corrcoef = corrcoef[~np.isnan(corrcoef)]\n",
    "            else:\n",
    "                raise ValueError(f\"dim value erro: dim={dim}\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Correlation Name Error: {corr}\")\n",
    "\n",
    "\n",
    "    corrcoef = torch.tensor(corrcoef)\n",
    "\n",
    "    highest_indices = torch.topk(corrcoef, int(0.1 * len(corrcoef)), largest=True).indices\n",
    "    corrcoef10 = corrcoef[highest_indices] \n",
    "\n",
    "    return corrcoef, corrcoef10\n",
    "\n",
    "\n",
    "def plot_corrcoef_boxplot(predicted_ess, original_ess, save_path=None,\n",
    "                           merge_actionable_and_top10 =False, title=\"\", show=False):\n",
    "    \n",
    "\n",
    "    # Calculate correlation coefficients\n",
    "    drugs_corrcoef, drugs_corrcoef10 = get_corrcoef(predicted_ess, original_ess, dim=0)\n",
    "    drugs_corrcoef, drugs_corrcoef10 = np.nan_to_num(drugs_corrcoef, nan=0.0, posinf=0.0, neginf=0.0), np.nan_to_num(drugs_corrcoef10, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    cell_corrcoef, cell_corrcoef10 = get_corrcoef(predicted_ess, original_ess, dim=1)\n",
    "    cell_corrcoef, cell_corrcoef10 = np.nan_to_num(cell_corrcoef, nan=0.0, posinf=0.0, neginf=0.0), np.nan_to_num(cell_corrcoef10, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "\n",
    "        data = [drugs_corrcoef, drugs_corrcoef10, cell_corrcoef, cell_corrcoef10]\n",
    "        labels = ['Drugs', 'Drugs 10%', 'Cells', 'Cells 10%']\n",
    "\n",
    "        bp = plt.boxplot(data, labels=labels,\n",
    "                         patch_artist=True, showfliers=True, notch=True, meanline=True)\n",
    "        \n",
    "        bp['boxes'][0].set_facecolor(\"lightblue\")\n",
    "        bp['boxes'][1].set_facecolor(\"lightblue\")\n",
    "        bp['boxes'][2].set_facecolor(\"brown\")\n",
    "        bp['boxes'][3].set_facecolor(\"brown\")\n",
    "        \n",
    "    plt.ylabel('Correlation')\n",
    "    plt.title(f'{title}', fontsize=10)\n",
    "    # Ensure the correlation values being displayed are finite\n",
    "    if np.isfinite(np.median(drugs_corrcoef)):\n",
    "        plt.text(0.65, np.median(drugs_corrcoef), round(np.median(drugs_corrcoef), 2), fontsize=8, rotation=90)\n",
    "\n",
    "    if np.isfinite(np.median(drugs_corrcoef10)):\n",
    "        plt.text(1.65, np.median(drugs_corrcoef10), round(np.median(drugs_corrcoef10), 2), fontsize=8, rotation=90)\n",
    "\n",
    "    if np.isfinite(np.median(cell_corrcoef)):\n",
    "        plt.text(2.65, np.median(cell_corrcoef), round(np.median(cell_corrcoef), 2), fontsize=8, rotation=90)\n",
    "\n",
    "    if np.isfinite(np.median(cell_corrcoef10)):\n",
    "        plt.text(3.65, np.median(cell_corrcoef10), round(np.median(cell_corrcoef10), 2), fontsize=8, rotation=90)\n",
    "\n",
    "    # if np.isfinite(np.median(actionable)) and not merge_actionable_and_top10:\n",
    "    #     plt.text(4.65, np.median(actionable), round(np.median(actionable), 2), fontsize=8, rotation=90)\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "689d99b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ess.load_state_dict(torch.load(\"baselines/VAE_baseline/model_ess_var.pt\"))\n",
    "model_exp.load_state_dict(torch.load(\"baselines/VAE_baseline/model_exp_var.pt\"))\n",
    "y_ess_pred, y_ess_true = evaluate(test_loader, model_ess, model_exp, device=device)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fda2dfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the predicted and true values as df\n",
    "pd.DataFrame(y_ess_pred, columns=ess_gene_list).to_csv(\"baselines/VAE_baseline/y_ess_pred_var.csv\")\n",
    "pd.DataFrame(y_ess_true, columns=ess_gene_list).to_csv(\"baselines/VAE_baseline/y_ess_true_var.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ce627501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_corrcoef_boxplot(y_ess_pred, y_ess_true,\n",
    "                     save_path=\"baselines/VAE_baseline/corrcoef_boxplot_var.pdf\",\n",
    "                      title=\"\",\n",
    "                      show=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
